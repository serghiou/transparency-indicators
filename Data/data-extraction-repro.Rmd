---
title: 'HRP 262: Final'
author: "Stylianos Serghiou"
date: '`r format(Sys.time(), "%d/%m/%Y")`'
output:
  html_document:
    df_print: kable
    highlight: tango
    theme: paper
    toc: yes
    toc_depth: 2
    toc_float: yes
  pdf_document:
    df_print: kable
    highlight: tango
    keep_tex: yes
    latex_engine: pdflatex
header-includes:
- \DeclareUnicodeCharacter{3B8}{~}
- \DeclareUnicodeCharacter{3B1}{~}
- \DeclareUnicodeCharacter{3B2}{~}
- \DeclareUnicodeCharacter{223C}{~}
- \DeclareUnicodeCharacter{2264}{~}
- \DeclareUnicodeCharacter{2265}{~}
---

<style>
p {

text-align: justify;
text-justify: interword;
padding: 0 0 0.5em 0

}
</style>

```{r knitr, include = FALSE}
# Load knitr
library(knitr)

# Define chunk options
knitr::opts_chunk$set(echo = TRUE
                      , warning   = FALSE 
                      , message   = FALSE
                      , comment   = NA
                      , fig.align = "center"
                      , linewidth = 91)

# Initiatialize hook
hook_output = knit_hooks$get("output")

# Hook to wrap output text when it exceeds 'n' using linewidth
knit_hooks$set(output = function(x, options) {
  
  if (!is.null(n <- options$linewidth)) {
    x = knitr:::split_lines(x)
    
    if (any(nchar(x) > n)) # wrap lines wider than 'n' 
      x <- strwrap(x, width = n)
      x <- paste(x, collapse = "\n")
  }
  hook_output(x, options)
})

# For more knitr options visit: https://yihui.name/knitr/options/
# and his github page: https://github.com/yihui/knitr-examples

# To update the package use:
# library(devtools); setwd("/Users/Stelios/"); install("serghiouTemplates")
```

```{r setup}
# Packages
library(readxl)
library(reutils)
library(RISmed)
library(magrittr)
library(dplyr)
library(reutils)
library(pbapply)
library(XML)       # for xmlParse, etc.
library(RCurl)     # for getURL
library(jsonlite)  # for fromJSON
library(rcrossref) # for id_converter, cr_citation_count, etc.
library(reticulate)
library(rvest)
library(parallel)

source("utils.R")
source("bioRxiv_Functions.R")

dat <- read_xlsx("jpai_rand1000.xlsx")
```

```{r prep}
open_access <- !is.na(dat$PMCID)
pmcid <- dat$PMCID[open_access]
pmid <- dat$PMID
id <- dat$Order

# Links
links <- sprintf("https://www.ncbi.nlm.nih.gov/pmc/articles/%s/pdf/main.pdf",
                 pmcid)

destination <- sprintf("./papers/%04d-PMID%s-%s.pdf",
                       id[open_access], pmid[open_access], pmcid) 

# seq_along(links)
errors <- c()
for (i in 1:length(links)) {
  
   trial <- tryCatch(download.file(links[i], destfile = destination[i]), 
                     error = function(e) e)

  # Return value
  if (inherits(trial, "error")) {
    errors <- c(errors, i)
  } 
  Sys.sleep(runif(1, 1, 5))
  if (1 %% 20 == 0) {print(sprintf("i = %s", i))}
}
```


```{r}
searchTerms <- paste0(pmid, "[uid]")

# Initialize
toc <- list()

# Progress bar
pbar  <- txtProgressBar(min = 0, max = length(searchTerms), style = 3)
kpval <- 0

# (use k instead of i b/c loops in fetchArticles are using 'i')
for (k in 601:length(searchTerms)) {
  
  frame <- fetchArticles(searchTerms[k])
  
  # Add group variable
  frame$Group <- k
    
  # Store
  toc[[k]] <- frame
  
  # Progress bar
  kpval <- kpval + 1
  setTxtProgressBar(pbar, kpval)
}

# Create dataset
pubmed <- do.call(rbind.data.frame, toc)

# Save
export_xlsx(pubmed, "pubmed-info.xlsx")
```


## Fetching closed articles

Let us first try a few of the Python packages.

```{r}
use_condaenv("r-reticulate")
```

Now let us try running some of the packages developed for Python for extracting websites from PubMed.

```{python}
import metapub
from metapub import FindIt
src = FindIt('18381613')
print(src.url)
```

SciHub API

```{r}
# Null DOIs and no PMCID
ind <- pubmed$DOI != "NULL" & is.na(dat$PMCID)
closed_doi  <- pubmed$DOI[ind] 
closed_id   <- pubmed$Row[ind] 
closed_pmid <- pubmed$PMID[ind] 

# Construct websites
doi_link <- paste0("http://sci-hub.is/", closed_doi)

# Initialize vectors
is_miss <- c()
is_full <- c()
css.location <- "#article iframe"

# Function to extract PDF links
# I am not using a function because connection keeps dropping
pbar  <- txtProgressBar(min = 0, max = length(doi_link), style = 3)
kpval <- 0

for (i in seq_along(doi_link)) {
  
  pdf_link <- tryCatch(read_html(doi_link[i]), error = function(e) e)

  # Return value
  if (inherits(pdf_link,  "error")) {
    is_miss <- append(is_miss, i)
    next
  }

  pdf_link %<>% 
    html_node(css = css.location) %>% 
    gsub("^.+//(.+)#.+$", "\\1", .)
  
  if (grepl(".sci", pdf_link)) {
    is_miss <- append(is_miss, closed_id[doi_link %in% doi_link[i]])
  } else {
    is_full %<>% append(pdf_link)
  }
  Sys.sleep(1)
  kpval  <-  kpval + 1
  setTxtProgressBar(pbar, kpval)
}

```


## Save

```{r save}
# Save
save(list = ls(), file = "output.RData")
```


## Documentation

### Session Info

```{r session_info}
print(sessionInfo(), locale = F)
```

### References

```{r refs}
(.packages()) %>% sort %>% lapply(citation) %>% lapply(c) %>% unique
```
